{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a88c312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'MiniChain'...\n",
      "remote: Enumerating objects: 1556, done.\u001b[K\n",
      "remote: Counting objects: 100% (38/38), done.\u001b[K\n",
      "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
      "remote: Total 1556 (delta 28), reused 25 (delta 25), pack-reused 1518\u001b[K\n",
      "Receiving objects: 100% (1556/1556), 54.59 MiB | 13.76 MiB/s, done.\n",
      "Resolving deltas: 100% (982/982), done.\n"
     ]
    }
   ],
   "source": [
    "!pip install -q git+https://github.com/srush/MiniChain\n",
    "!git clone https://github.com/srush/MiniChain; cp -fr MiniChain/examples/* . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "868e1c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = \"\"\"\n",
    "### Word Problem Solver\n",
    "\n",
    "Chain that solves a math word problem by first generating and then running Python code. [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/srush/MiniChain/blob/master/examples/math_demo.ipynb)\n",
    "\n",
    "(Adapted from Dust [maths-generate-code](https://dust.tt/spolu/a/d12ac33169))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64f4d12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from minichain import show, prompt, OpenAI, Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1ea8d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@prompt(OpenAI(), template_file=\"math.pmpt.tpl\")\n",
    "def math_prompt(model, question):\n",
    "    \"Prompt to call GPT with a Jinja template\"\n",
    "    return model(dict(question=question))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f44dc222",
   "metadata": {},
   "outputs": [],
   "source": [
    "@prompt(Python(), template=\"import math\\n{{code}}\")\n",
    "def python(model, code):\n",
    "    \"Prompt to call Python interpreter\"\n",
    "    code = \"\\n\".join(code.strip().split(\"\\n\")[1:-1])\n",
    "    return int(model(dict(code=code)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64a35ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def math_demo(question):\n",
    "    \"Chain them together\"\n",
    "    return python(math_prompt(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e68b91ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/bing/opt/anaconda3/envs/web_hw/lib/python3.9/site-packages/gradio/routes.py\", line 437, in run_predict\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/Users/bing/opt/anaconda3/envs/web_hw/lib/python3.9/site-packages/gradio/blocks.py\", line 1352, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/Users/bing/opt/anaconda3/envs/web_hw/lib/python3.9/site-packages/gradio/blocks.py\", line 1087, in call_function\n",
      "    raise ValueError(\"Need to enable queue to use generators.\")\n",
      "ValueError: Need to enable queue to use generators.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/bing/opt/anaconda3/envs/web_hw/lib/python3.9/site-packages/gradio/routes.py\", line 437, in run_predict\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/Users/bing/opt/anaconda3/envs/web_hw/lib/python3.9/site-packages/gradio/blocks.py\", line 1352, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/Users/bing/opt/anaconda3/envs/web_hw/lib/python3.9/site-packages/gradio/blocks.py\", line 1087, in call_function\n",
      "    raise ValueError(\"Need to enable queue to use generators.\")\n",
      "ValueError: Need to enable queue to use generators.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/bing/opt/anaconda3/envs/web_hw/lib/python3.9/site-packages/gradio/routes.py\", line 437, in run_predict\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/Users/bing/opt/anaconda3/envs/web_hw/lib/python3.9/site-packages/gradio/blocks.py\", line 1352, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/Users/bing/opt/anaconda3/envs/web_hw/lib/python3.9/site-packages/gradio/blocks.py\", line 1087, in call_function\n",
      "    raise ValueError(\"Need to enable queue to use generators.\")\n",
      "ValueError: Need to enable queue to use generators.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/bing/opt/anaconda3/envs/web_hw/lib/python3.9/site-packages/gradio/routes.py\", line 437, in run_predict\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/Users/bing/opt/anaconda3/envs/web_hw/lib/python3.9/site-packages/gradio/blocks.py\", line 1352, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/Users/bing/opt/anaconda3/envs/web_hw/lib/python3.9/site-packages/gradio/blocks.py\", line 1087, in call_function\n",
      "    raise ValueError(\"Need to enable queue to use generators.\")\n",
      "ValueError: Need to enable queue to use generators.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/bing/opt/anaconda3/envs/web_hw/lib/python3.9/site-packages/gradio/routes.py\", line 437, in run_predict\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/Users/bing/opt/anaconda3/envs/web_hw/lib/python3.9/site-packages/gradio/blocks.py\", line 1352, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/Users/bing/opt/anaconda3/envs/web_hw/lib/python3.9/site-packages/gradio/blocks.py\", line 1087, in call_function\n",
      "    raise ValueError(\"Need to enable queue to use generators.\")\n",
      "ValueError: Need to enable queue to use generators.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/bing/opt/anaconda3/envs/web_hw/lib/python3.9/site-packages/gradio/routes.py\", line 437, in run_predict\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/Users/bing/opt/anaconda3/envs/web_hw/lib/python3.9/site-packages/gradio/blocks.py\", line 1352, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/Users/bing/opt/anaconda3/envs/web_hw/lib/python3.9/site-packages/gradio/blocks.py\", line 1087, in call_function\n",
      "    raise ValueError(\"Need to enable queue to use generators.\")\n",
      "ValueError: Need to enable queue to use generators.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/bing/opt/anaconda3/envs/web_hw/lib/python3.9/site-packages/gradio/routes.py\", line 437, in run_predict\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/Users/bing/opt/anaconda3/envs/web_hw/lib/python3.9/site-packages/gradio/blocks.py\", line 1352, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/Users/bing/opt/anaconda3/envs/web_hw/lib/python3.9/site-packages/gradio/blocks.py\", line 1087, in call_function\n",
      "    raise ValueError(\"Need to enable queue to use generators.\")\n",
      "ValueError: Need to enable queue to use generators.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "gradio = show(math_demo,\n",
    "              examples=[\"What is the sum of the powers of 3 (3^i) that are smaller than 100?\",\n",
    "                        \"What is the sum of the 10 first positive integers?\",],\n",
    "                        # \"Carla is downloading a 200 GB file. She can download 2 GB/minute, but 40% of the way through the download, the download fails. Then Carla has to restart the download from the beginning. How load did it take her to download the file in minutes?\"],\n",
    "              subprompts=[math_prompt, python],\n",
    "              out_type=\"json\",\n",
    "              description=desc,\n",
    "              )\n",
    "if __name__ == \"__main__\":\n",
    "    gradio.launch()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d51981ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "desc = \"\"\"\n",
    "### Chat\n",
    "\n",
    "A chat-like example for multi-turn chat with state. [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/srush/MiniChain/blob/master/examples/chat.ipynb)\n",
    "\n",
    "(Adapted from [LangChain](https://langchain.readthedocs.io/en/latest/modules/memory/examples/chatgpt_clone.html)'s version of this [blog post](https://www.engraved.blog/building-a-virtual-machine-inside/).)\n",
    "\n",
    "\"\"\"\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1963f03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, replace\n",
    "from typing import List, Tuple\n",
    "from minichain import OpenAI, prompt, show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d6823fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "MEMORY = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4adc54c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class State:\n",
    "    memory: List[Tuple[str, str]]\n",
    "    human_input: str = \"\"\n",
    "\n",
    "    def push(self, response: str) -> \"State\":\n",
    "        memory = self.memory if len(self.memory) < MEMORY else self.memory[1:]\n",
    "        return State(memory + [(self.human_input, response)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0cef87a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@prompt(OpenAI(), template_file=\"chat.pmpt.tpl\")\n",
    "def chat_prompt(model, state: State) -> State:\n",
    "    out = model(state)\n",
    "    result = out.split(\"Assistant:\")[-1]\n",
    "    return state.push(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b3ca6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    \"ls ~\",\n",
    "    \"cd ~\",\n",
    "    \"{Please make a file jokes.txt inside and put some jokes inside}\",\n",
    "    \"\"\"echo -e \"x=lambda y:y*5+3;print('Result:' + str(x(6)))\" > run.py && python3 run.py\"\"\",\n",
    "    \"\"\"echo -e \"print(list(filter(lambda x: all(x%d for d in range(2,x)),range(2,3**10)))[:10])\" > run.py && python3 run.py\"\"\",\n",
    "    \"\"\"echo -e \"echo 'Hello from Docker\" > entrypoint.sh && echo -e \"FROM ubuntu:20.04\\nCOPY entrypoint.sh entrypoint.sh\\nENTRYPOINT [\\\"/bin/sh\\\",\\\"entrypoint.sh\\\"]\">Dockerfile && docker build . -t my_docker_image && docker run -t my_docker_image\"\"\",\n",
    "    \"nvidia-smi\"\n",
    "]\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "382516ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/bing/opt/anaconda3/envs/web_hw/lib/python3.9/site-packages/gradio/routes.py\", line 437, in run_predict\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/Users/bing/opt/anaconda3/envs/web_hw/lib/python3.9/site-packages/gradio/blocks.py\", line 1352, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/Users/bing/opt/anaconda3/envs/web_hw/lib/python3.9/site-packages/gradio/blocks.py\", line 1087, in call_function\n",
      "    raise ValueError(\"Need to enable queue to use generators.\")\n",
      "ValueError: Need to enable queue to use generators.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "gradio = show(lambda command, state: chat_prompt(replace(state, human_input=command)),\n",
    "              initial_state=State([]),\n",
    "              subprompts=[chat_prompt],\n",
    "              examples=examples,\n",
    "              out_type=\"json\",\n",
    "              description=desc,\n",
    ")\n",
    "if __name__ == \"__main__\":\n",
    "    gradio.launch()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c78eed0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
